
\documentclass[twocolumn]{article}
    \usepackage{amsmath}

\usepackage{graphics}
\usepackage{grffile}
\title{\bf Moment closure methods} 
\newcommand\p[2]{\frac{\partial #1}{\partial #2}}
\newcommand\pd[2]{\frac{\partial^2 #1}{\partial #2^2}}
\newcommand\pn[3]{\frac{\partial^#1 #2}{\partial #3^#1}}


\newcommand\dud{\left(\frac{1}{N_B}-\frac{1}{N_A}\right)}


\author{Simon Gravel, McGill University, Montreal, Canada }
%\ead{xxx}

\begin{document}

\maketitle
\begin{abstract}
We present a  moment closure approach to solving linear PDEs in cases where coefficients are polynomial and time-dependant. Closure approximation is achieved through linear moment matching.  
 The proposed application is the diffusion equation in genetics. 
\end{abstract}


Consider a partial differential equation of the form 

$$\p{\phi(x,t)}{t}= \sum_{n=1}^N \pn{n}{}{x}\left(P^n(x) \phi(x,t)\right) $$

By integrating each side with $\int x^k dx$, we get

$$\p{\int x^k dx \phi(x,t)}{t}= \sum_{n=1}^N \int x^k dx \pn{n}{}{x}\left(P^n(x) \phi(x,t)\right).$$

The left-hand side gives us the time-derivative of $\mu_k$, the $k$th moment of $\phi$: 
$$\mu_k\equiv \int_0^1 x^k  \phi(x,t) dx.$$

The right-hand side can be expressed in terms of the moments of $\phi$, and some boundary terms, via integration by parts:
 
 $$\dot \mu_k =\mbox{boundary terms}+\sum_{n=1}^N (-1)^n x^{k-n} P^n(x) \phi(x).$$
 
 The right-hand side term can be expressed as 
 
$$\dot \mu_k = \sum_{i=1}^p \mu_i.$$

If $p\leq k$, for all $k$, the first moments of the equation can be solved as a finite set of coupled ODEs. 

For example, consider the one-dimensional diffusion equation.    

[[The following is verbatim from efficacy paper

\begin{equation}
\begin{split}
\p{\phi(x,t)}{t}\simeq &\frac{1}{4 N} \pd{}{x} x(1-x) \phi(x,t) \\&-s \p{}{x} \left(h+(1-2h)x \right)x (1-x) \phi(x,t) \\&+ 2 N u \delta(x-\frac{1}{2 N}),
\end{split}
\label{diff2}
\end{equation}
 



To obtain evolution equations,  we integrate both sides of equation \eqref{diff2} using $\int_{0^+}^{1^-} dx x^k$. The left-hand side gives
\begin{equation}
\begin{split}
\int_{0^+}^{1^-} dx x^k \p{\phi(x,t)}{t} = \p{\int_{0^+}^{1^-} dx x^k \phi(x,t)}{t}\\= \dot \mu_k -\dot  K_0 \delta_{k,0}-\dot K_1 .
\end{split}
\label{RHS}
\end{equation}
The right-hand side can be evaluated by integrating by parts. 
For $k=0$, this yields 
 $$\dot \mu_0= 2 N u-\frac{\phi(0,t) + \phi(1,t) }{4N} + \dot K_0+ \dot K_1,$$ where $\phi(0,t)$ and $\phi(1,t)$ 

are defined by continuity from the open interval $(0,1)$. In other words, they do not include fixed sites.  
Because the number of sites is constant ($\dot \mu_0=0$) and the diffusion equation continuous, we require
\begin{equation}
\begin{split}
 \dot K_0&=-2 N u+\frac{\phi(0,t)}{4N}, \\  
 \dot K_1&=\frac{\phi(1,t)}{4N}.
\end{split}
\label{Ks}
\end{equation} 
These equations are equivalent to Equations 3.18 and 3.19 in \cite{Kimura:1964jm}. We will use these expressions in our integration of equation \eqref{diff2} with $\int_{0^+}^{1^-} dx x^k$.  We use the left-hand-side expression obtained in Equation \eqref{RHS} and integrate the right-hand side by parts. This gives us an expression for $\dot \mu_k.$
\begin{equation}
\dot \mu_k= \frac{k (k-1)}{8N} \pi_{k-1}+ \frac{s k}{4} \Gamma_{k,h}+  \frac{u}{(2 N)^{k-1}} , 
\label{muk}
\end{equation}
where $\pi_k$ and $\Gamma_{k,h}$ are: 
$$\pi_k=2 (\mu_k-\mu_{k+1})$$ and $$\Gamma_{k,h}=2\left(h \pi_k+(1-2h)\pi_{k+1}\right).$$ These are functions of the moments $\mu$ and can therefore be thought of as measures of the shape of the frequency distribution $\phi$. 

The first term in \eqref{muk} represents the effect of drift, the second term the effect of selection, and the third term the effect of mutation.  
 
 
For example, if $k=1$ and $s=0$, we get 
 \begin{equation}
 \begin{split}
 \dot \mu_1 &= u\\
 \dot \mu_k &= \frac{k (k-1)}{4 N} (\mu_{k-1}-\mu_{k})+\frac{u}{(2N)^{k-1}} , ~~~~k>1 
 \end{split}
 \end{equation}  

We can chose an arbitrary cutoff $K$ and solve a finite set of ODEs for all the $\mu_k$ with $k<K$. 

\section{Obtaining $\Phi$ from the $\mu_k$}

If we are interested in the expected frequency spectrum $\Phi_n (i)$ for $n$ haploid samples, we can use

$\Phi_n (i) = \int dx {n \choose i} x^i (1-x)^{n-i}\phi(x)$

to write the frequency spectrum as a linear combination of the $\mu_k$. The danger with this approach is that it is numerically unstable: The $(1-x)^n$ term contains large and rapidly oscillating terms which will amplify any numerical error. This was observed in Evans, Shvets, Slatkin (2009). They argued around Eq. 51 that writing the expression directly for the $\Phi$ could help alleviate the problem, but that it actually just threw the problem of numerical instability around and didn't really solve it. 

There are a few potential ways around this issue:

\begin{itemize}
\item Write the PDE equations directly for the $\Phi_n (i)$
\item Use an approximate but stable expression for $\Phi_n (i)$
\end{itemize}

\subsection{Solving for $\Phi_n(i)$ directly?}
Here we are trying to solve $\Phi_n(i)$ directly. This sounds like it should be impossible: we need to know the distribution of the very rare stuff to predict the number of singletons. 


$$ \dot \Phi_n(i) = \int_0^1 {n\choose i}  x^i (1-x)^{n-i} \dot \phi dx$$
with 

$$ w_i=  {n\choose i}  x^i (1-x)^{n-i}.$$

Note $\Phi_n(0)$ contains the fixed terms: Because we are integrating from $0$ to $1$, the delta functions at the boundary in $\phi$ are included. When integrating by parts, these can cause issues. Write $\phi = \phi_s + K_0 \delta(x) + K_1 \delta(x-1)$, where $\phi_s$ is the smooth component of $\phi$  

$$ \dot \Phi_n(i) = \dot K_0 \delta_{i,0} +\delta_{i,n} \dot K_1+ \int_0^1 {n\choose i}  x^i (1-x)^{n-i} \dot \phi_s dx$$


\begin{equation}
\begin{split}
 \dot \Phi_n(i) =& \dot K_0 \delta_{i,0} +\delta_{i,n} \dot K_1+ \int_0^1 w_i \\ &\left( \frac{1}{4 N} \pd{}{x} x(1-x) \phi_s(x,t) + 2 N u \delta(x-\frac{1}{2 N})\right) 
\end{split}
\end{equation}
The delta term contributes 

$$u {n\choose i}  \frac{1}{2N}^{i-1} \left(1-\frac{1}{2N}\right)^{n-i}.$$ This is very small for anyone who is not the zerotons and the 1-tons. 

The second derivative contributes 
$$\frac{1}{4 N} \left(\delta_{i,0} \phi(0,t) +\delta_{i,n} \phi(1,t) +\int \pd{w_i}{x} x (1-x) \phi(x,t)\right).$$

The boundary terms tell us the rate of fixation of alleles. We'd like to avoid computing the actual values of $\phi(0,t)$. 

\begin{equation}
\begin{split}
\pd{w_i}{x} =&  {n\choose i} \left( i (i-1)  x^{i-2} (1-x)^{n-i} - 2 i (n-i ) x^{i-1} (1-x)^{n-i-1}\right.\\& \left. +(n-i) (n-i-1) x^{i} (1-x)^{n-i-2} \right)
\end{split}
\end{equation}


The second derivative contributes 
\begin{equation}
\begin{split}
\frac{1}{4 N} (\delta_{i,0} \phi(0,t) +\delta_{i,n} \phi(1,t) \\
+(i-1)(n-i+1) \Phi_s (i-1) \delta_{i\geq 2}\\
-2 i(n-i) \Phi_s(i) \delta_{1\leq i \leq n-1}  \\ 
+ (n-i-1)(i+1) \Phi_s(i+1)  \delta_{i\leq n-2} )
\end{split}
\end{equation}
where $\Phi_s$ represent the "unfixed" sites. Conveniently, fixed sites contribute only to $\Phi_0$ and $\Phi_n$, so that evolution equations can be written in terms of the $\Phi_i(n)$ with $1\leq i\leq n$ only. These are ver simple expressions! For $\dot \Phi_n(1)$, we get

$$ \dot \Phi_n(1)= n u  \left(1-\frac{1}{2N}\right)^{n-1} -2 (n-1) \Phi(1) + 2 (n-2) \Phi_s(2)  $$

Contrary to the claims of Evans (2009), the oscillations are very well-bounded--it is nothing more than computing a numerical derivative. This should be a trivial computation. Unless I messed up somehow. 



\subsection{Breaking the closure.}
Under selection, the equations are not closed anymore. For $h=1/2,$ we have $\dot \mu_k$ depends on $ \mu_{k+1}$. For the h, it depends on  $\mu_{k+1} as well.$ We need to find a way to close the moment equation. The classical method would be to truncate the moments and set $\mu_i=0$ for $i> K$ for some arbitrary cutoff $K$. 

The equations and ideas are most easily expressed in terms of the $\mu_k$, but what we have seen above compels us to believe that we want all expressions in terms of the $\Phi_n(i).$ If we look at the contribution of selection to  $\dot \Phi_n(i),$ we get: 

 $$-s \int {n \choose i} x^i (1-x)^{n-i} \p{}{x} \left(h+(1-2h)x \right)x (1-x) \phi(x,t). $$
 
 Once again the boundary terms vanish for $1\leq i \leq n-1$, and we get  

 $$s \int {n \choose i} \left(i (1-x) - (n-i) x\right)    x^{i} (1-x)^{n-i}  \left(h+(1-2h)x \right) \phi(x,t). $$
To see the magic, use $\left(h+(1-2h)x \right) = h (1-x) + (1-h) x$. Now all terms will be homogeneous in powers of $x, (1-x)$, and we can write everything in terms of frequency spectrum terms in a sample of size $n + 2$. Note that for $h = 1/2,$ the term  $h (1-x) + (1-h) x$ reduces to $1/2$, and therefore we have terms that are homogeneous of order $n+1$ rather than $n+2$. Here again, we only have a small number of terms. 

We are now left with a minor conundrum: drift contributions are expressed in terms of $\Phi_n$, selection contributions in terms of the $\Phi_{n+2},$ or at least of the $\Phi_{n+1}.$ It's easy to convert the $\Phi_n$ terms in higher-order terms: we simply need to multiply in the integral by $1= x + (1-x).$. However, we do not know the terms of order  $\Phi_{n+2},$ and calculating them would require higher order terms. How do we break the cycle? 

\section{Jackknife to the rescue}

In \cite{jackknife}, I showed that the number of variants in a sample of size $n+1$ can be easily predicted from a sample of size $n$ using a jackknife approach. In this approach, we use a linear combination of the $\Phi_n(i)$ to estimate the number of missed variants, $V= \int (1-x)^n - (1-x)^{n+1}  \phi(x).$ This works very well, because we can pick the coefficients in the linear combination in such a way that it is exactly equal to the missed variants $V$, as long as $\phi(x)$ belongs to a given family of functions. The general idea is that, unless $\phi(x)$ is really wild, there is a rather tame relationship between the number of singletons in $n$ samples and the number of singletons in $n+1$ samples. A key to this approach is to find a good approximation to $x^i (1-x)^{n-i+2}$ using a linear combination of the $x^j (1-x)^{n-j}.$ In addition, we require that the number of terms in the linear combination is small, so that the dreaded oscillations do not occur. It is impossible to get a perfect approximation of course, but $x (1-x)^{20}$ is not all that different of $ x (1-x)^{22}.$ If we compare  $0.9 x (1-x)^{20}$ to  $x (1-x)^{22},$ there is not much of a difference. The difference gets even slimmer as $n$ gets larger. What's more, we don't need these weights to be exactly equal to each other: we simply need their integrals to vanish when applied to a suitable function. We can choose the coefficient so that the two curves are reasonably close, and so that the errors cancel out when applied to a given function, say $\phi(x)=1/x$. We can make the approximation even better by considering a slightly higher-order jackknife. for example, if we approximate $ x (1-x)^{22} $ as  $\alpha x (1-x)^{20} + \beta x^2 (1-x)^{19},$ we can build a better approximation that will be exact for any $\phi= a/x +b/(1-x).$ This would be akin to a second-order jackknife. We can keep adding terms, but at some point we will run into numerical instabilities. Because we are typically extrapolating from order $n$ to order $n+2$, our basis functions will be good approximations for the target basis functions, and our approximation will be excellent with a small number of terms.  I'm expecting that first- to third- order jackknife will be sufficient. 

\section{Central moment closure problem}

The approach presented above can be applied to any open moment equation, as long as $\mu_k$ depends on $\mu_{k+i}$ for $i$ not too large. As it turns out, this is a rather common occurrence. In this case, we'd simply approximate $x^{k+1}$ using a linear combination of the $x^k,x^{x-1},x^{k-2},...,x^{k+1-p},$ where p is the order of the jackknife approximation. The beauty of this approach is that the linear approximation actually "folds" the higher order terms into the lower-order linear equation, rather than simply truncating them, at no computational cost. 
  
\section{Estimating $\phi$ using derivatives and a weak smoothness approximation}
We'd like to use our knowledge of $\phi$ a little bit more. In particular, our knowledge that $\phi$ is a relatively friendly function. To estimate $\phi(x_0),$ we express it as
$$\phi(x_0)=\int \delta(x-x_0) \phi (x).$$
If we can estimate  $ \delta(x-x_0)$ using a linear combination of the $x^0,...,x^k$, then we can approximate this integral. For example, $\frac{x^{10} (1-x)^{10}}{C}$ might do a decent job at estimating $\delta(x-0.5)$, for a suitable constant $C$. However, estimating $\delta$ might just be the hardest job for a polynomial basis. in fact, $\frac{x^{10} (1-x)^{10}}{C}$ does an ok job, but nothing glorious. To improve upon this, we observe that if $\phi(x)$ had been a constant, over the range where $\frac{x^{10} (1-x)^{10}}{C}$ is substantial, our estimate would have been great. Our estimates are not good because $\phi(x)$ varies on the interval. 

Suppose that $w(x)$ is a normalized approximation to $\delta (x-x_0)$  we can simply expand 
$$\int w(x) \phi(x) = \int w(x) \left(\phi(x_0)+ (x-x_0) \phi' (x_0) + \frac{(x-x_0)^2}{2} \phi'' (x_0)\right)$$

and 

$$\int w(x) \phi(x) = \phi(x_0)+ \int w(x) \left((x-x_0) \phi' (x_0) + \frac{(x-x_0)^2}{2} \phi'' (x_0)\right)$$

Therefore $$\int w(x) \left((x-x_0) \phi' (x_0) + \frac{(x-x_0)^2}{2} \phi'' (x_0)\right)$$
provides a second-order correction to our initial estimate $\int w(x) \phi(x)$.  To get the correction, we assumed that $\phi$ can be approximated by a quadratic function \emph{over the range where $w(x)$ is substantial}. If $w$ is peaked enough, this approximation should be good. 


[[No!-- We might be tempted to integrate by parts. Assuming that $w(x)$ vanishes at the boundary because I am lazy right now. We can write: 


$$\int w(x) \phi(x) = \int \left(w(x)- \p{(x-x_0) w(x)}{x}+\pd{(x-x_0)^2 w(x)}{x}   \right) \phi(x_0).$$

This way we could potentially produce a better $w(x)$. In fact, setting  $- \p{(x-x_0) w(x)}{x}+\pd{(x-x_0)^2 w(x)}{x} =0 $ would be a possible way of choosing our $w(x)$. 

However, adding all these derivatives of $w$ has the potential of creating additional oscillations. 
]]
Another option is simply to solve directly for the moments of $\phi'(x)$. and $\phi''(x)$. Solving for both would increase our computational time by a factor of three. 

However, we would now have a numerically stable way of estimating the underlying $\phi$ using only a weak assumption on the form of $\phi$.  Simulations look pretty good. (scratch.nb) There may be a better way of estimating the correction. For example, we could estimate $\phi'(x_0)$ using  $\frac{\phi(x_0+\delta)-\phi(x_0)}{\Delta}$, and so forth. I believe that oscillations will remain controlled in this case. 
  

\section{Self-consistent approach}
If I can estimate the $\phi(x)$ as mentioned above, I can choose my jackknife parameters to have exact error cancellation for this particular function. I can solve again with the new parameters, which should lead to a reduced error. 

However, estimating $\phi$ at each position and integrating over it sounds horribly ineffective. Instead, if we are dealing with the relatively peaked functions $x^i (1-x)^{n-i}$, and trying to extrapolate to $x^j (1-x)^{n+2-j}$  we could do the following. First, assume that $\phi(x)$ is smooth around $x_0=\frac{j}{n+2}$, enough that we can get 
$\Phi_n(i-1)$, $\Phi_n(i)$, $\Phi_n(i+1)$ by expanding $\phi$ around the same $x_0$. Since we know the $\Phi$, we can then solve for $\phi(x_0)$, $\phi'(x_0),$ $\phi''(x_0)$.  That gives us the local shape of the function. We can then pick a first-order jackknife that is perfect for that functional shape. For this to work, it is important that the first-order jackknife weight remains peaked! This approach is convenient because the solution for the $\phi, \phi', \phi''$ is the solution to a simple linear equation, and the parameters are known. 

Another possibility, since we may run into issue around rare variants where large gradients abound, would be to use a padé approximant for $\psi(x)$. 

This approach might be particularly useful in high dimension. 

\section{Further reducing the number of terms to calculate}

For large sample sizes, do we really need to know the exact value of the function $\Phi_n(i)$ for each $i$ at each time point? If $n=100$, we could imagine that 

$\Phi_n(i)\simeq \frac{\Phi_n(i-1)+ \Phi_n(i+1)}{2}.$ If we can get away with such an approximation, we can reduce the computational burden substantially in high dimensions. The simple approximation proposed here could lead to a factor of 2 in 1-D, but a factor of 8 in 3-D. The more we approximate, the higher we risk running into numerical instabilities, but such an approach might be worth exploring. I could easily imagine that in a sample of size $1000$, we can use $100$ data points or even 20 and do a decent job a estimating the SFS. If that works, we just opened up the possibility of solving 5- dimensional problems, or solving 3-D ones in a matter of seconds. 

Once again, the interpolation can use a low-order jackknife formalism. For example, if we have $n=1000,$ we could decide to keep the "skeleton" $i=1,2,3,4,5,6,8,10,15,20,50,100,200,500,800,900,950,980,985,990,992,994,995,996,997,998,999$, for a total of 27 terms.  We can probably do a really excellent job. 

Since we can express all the $\Phi_n$ as a linear function of the skeleton ones, ideally a non-alternating one (i.e., something like an average), we can probably also do a superb job of estimating the higher-order $\Phi_{n+2}$. In principle, we could reconstruct the missing $\Phi_n$, or we could directly jackknife the $\Phi_{n+2}$ using the skeleton terms.  


Which interpolation scheme should we use? The fifth-order WENO scheme seems like a good option. 



\section{Many dimensions}

Considerations for the expansion to multiple dimensions. 

For the mutations, we can simply add the following to the diffusion equation:

$$\sum_i 2 N_i \delta(x_i-\frac{1}{2 N_i})\prod_{j\ne i} \delta(x_j)$$.


Now our statistics are 
$$\Phi^{\mathbf{n}}_{\mathbf{i}}= \int \prod_{j=1}^P { n_j \choose i_j} x_j^i (1-x_j)^{n_j-i_j} dx_j \phi(\mathbf{x}) $$ where $\mathbf{n}$ is a vector of sample sizes and $\mathbf{i}$ is a vector of counts of alternates for $P$ populations.   

In the diffusion equation, the moment equation for $\Phi^{\mathbf{n}}_{\mathbf{i}}$ is similar to the one-dimensional case: we can express it as a function of other moments $\Phi^{\mathbf{n'}}_{\mathbf{i'}},$ with $\mathbf{n'}$ close to $\mathbf{n'}$ and $\mathbf{i'}$ close to $\mathbf{n'}.$ The migration term prevents closure, just as the selection term did. In two-d, $\dot \Phi^{mn}_{ij}$ would depend on terms like $\Phi^{m+1,n-1}_{ij},$ but we know how to handle those from $\Phi^{m+1,n-1}_{ij}.$ The boundary terms will be a little bit more messed up. 





\section{A nonlinear extension}

What if the pre is nonlinear? Can we still say something? Is the breakdown still useful?

 

   
 


 




 

 



  




 
 





  

\end{document}




 
