\documentclass[11pt]{article}

\usepackage[margin=2cm]{geometry}

\usepackage{amsmath,amsfonts,amssymb}
\usepackage{url}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{hyperref}

\usepackage[round]{natbib}
\renewcommand{\cite}{\citep}
\setlength{\bibhang}{0pt}

\setlength{\parindent}{0pt}
\setlength{\parskip}{10pt}

%\bibliographystyle{plos2009}

\usepackage{xspace}
\newcommand{\dadi}{$\partial$a$\partial$i\xspace}
\newcommand{\bolddadi}{$\boldsymbol{\partial}$a$\boldsymbol{\partial}$i\xspace}
\newcommand{\Nref}{\ensuremath{N_\text{ref}}\xspace}
\newcommand{\ms}{\emph{ms}\xspace}
\usepackage{color}
\newcommand{\comment}[1]{{\color{blue}APR: #1}}

\newcommand{\mold}{\texttt{mold}\xspace}

\usepackage{listings}
\lstset{
basicstyle=\ttfamily,
language=Python,
showstringspaces=False,
aboveskip=0pt,
captionpos=b,
belowskip=0pt
}
\newcommand{\py}[1]{\lstinline[breaklines=true,language=Python, showstringspaces=False]@#1@}
\newcommand{\ccode}[1]{\lstinline[breaklines=true,language=C, showstringspaces=False]@#1@}
\newcommand{\shell}[1]{\lstinline[breaklines=true, language=csh, showstringspaces=False]@#1@}

\newcommand{\E}{\mathbb{E}}

% For calibration, lines can be 60 characters long in
% lstlistings.
%\begin{lstlisting}
%*******************************************************
%\end{lstlisting}


\begin{document}
\title{\texttt{Moments LD} user manual\\
\normalsize  Corresponding to version 0.0.1}
\author{Aaron Ragsdale \\
Contact: aaron.ragsdale@mail.mcgill.ca}
\date{\today}
\maketitle

\tableofcontents

\clearpage

\renewcommand*{\lstlistlistingname}{Example code}
\lstlistoflistings

\clearpage

\section{Introduction to \mold}

Welcome to \texttt{moments.LD}, a program for simulating linkage disequilibrium statistics.
\texttt{moments.LD}, or \mold, can compute a large set of informative LD statistics for many populations, and performs likelihood-based demographic inference using those statistics.

There are three primary features of \mold to enable LD-based demographic inference: reading and parsing data, building demographic models, and inferring the parameters of those models by comparing model predictions to data.
Typically, we use biallelic SNP data, along with a recombination map, to compute two-locus statistics over a range of genetic distances.
We then use \mold to compute expectations for those statistics under the demographic models we want to test, which can include multiple populations with variable migration, splits and mergers, and population size changes.
Using a likelihood-based inference approach, we optimize those models to find the set of parameters that best fit the data.

I've tried to make parsing data and defining demographic models as painless as possible, though the complexity of the program does require some amount of script-writing and interaction.
Luckily, \mold is written in Python, a friendly and powerful programming language.
If you are already familiar with \dadi or \emph{moments}, or Python in general, you are in a good position to dive right in to \mold.
If you have limited Python experience, this manual should provide the background and examples to get you up to speed and productive with \mold.

%Finally, \mold is a living, breathing, evolving \comment{thing}

\subsection{Getting help and helping us}

Undoubtedly, there will be bugs.
If you find a bug in \mold, or more generally if you find certain aspects of the program to be unintuitive or difficult to use, I would appreciate the feedback.
Please submit a bug report at \url{https://bitbucket.org/simongravel/moments/issues}, and I will try to address the issue in a timely manner.
Similarly, if you have suggestions for improved functionality or feature requests, those can be submitted in the issues as well or you can contact me directly.

As we do our own research, \textit{moments} and \mold are constantly improving.
Our philosophy is to include any code we develop for our own projects that may useful to others.
If you develop \textit{Moments}-related code that you think might be useful to others, please let me know so I can include it with the main distribution.

\section{LD statistics}

Patterns of linkage disequilibrium (LD) are informative about evolutionary history, for example for inferring recent admixture events and population size changes or localizing regions of the genome that have experienced recent selective events.
LD is commonly measured as the covariance (or correlation) of alleles co-occurring on a haplotype.
The covariance ($D$) is
\begin{align*}
D = \text{Cov}(A,B) & = f_{AB} - pq \\ & = f_{AB}f_{ab} - f_{Ab}f_{aB},
\end{align*}
and the correlation ($r$) is
\begin{align*}
r = \frac{D}{\sqrt{p(1-p)q(1-q)}} .
\end{align*}
We think of expectations of these quantities as though we average over many realizations of the same evolutionary process, but in reality we have only a single observation for any given pair of SNPs.
Therefore in practice we take the averages of LD statistics over many independent pairs of SNPs.

$\E[D]$ is zero genome wide, so LD is often measured by the variance of $D$ ($\E[D^2]$) or the square correlation ($r^2$), where
\begin{align*}
r^2 = \frac{D^2}{p(1-p)q(1-q)}.
\end{align*}
Because it is difficult to compute expectations for $\E[r^2]$ under even simple evolutionary scenarios, and because it is difficult to accurately estimate $\widehat{r^2}$ from data, we use $\E[D^2$ and related statistics to compare model predictions for LD to data.

\subsection{Hill-Robertson statistics}

\citet{Hill1968} introduced a recursion for $\E[D^2]$ that allows for variable recombination rate between loci and population size changes over time.
To solve for $\E[D^2]$, this system requires additional LD statistics, which we call $Dz = D(1-2p)(1-2q)$ and $\pi_2 = p(1-p)q(1-q)$, where $p$ and $q$ are the allele frequencies at the left and right loci, respectively.
This system also relies on heterozygosity ($H$), so from this system we can compute the vector of statistics
$$y=\begin{pmatrix} \E[D^2] \\ \E[Dz] \\ \E[\pi_2] \\ \E[H] \end{pmatrix}.$$

Instead of computing $\E[r^2]$, which is an expectation of ratios, \citet{Hill1968} and \citet{Ohta1971} studied the related statistic $\sigma_D^2 = \frac{\E[D^2]}{\E[\pi_2]}$.
This statistic has the advantage that its expectation can be computed from the Hill-Robertson recursion, and we can accurately compute it from either phased or unphased data.

\subsection{Multi-population LD statistics}

In \citet{Ragsdale2018}, we extended the Hill-Robertson system to consider LD statistics for multiple populations.
Here, we can model population size changes, splits, mergers, and migration events (pulse or continuous).
The statistics we consider take the form
$$
\mathbf{z} = \begin{pmatrix} 
\E[D_i D_j] \\
\E[D_i z_{j,k}] \\
\E[{\pi_2}(i,j;k,l)] \\
\E[H_{i,j}]
\end{pmatrix},
$$

where $i,j,k,l$ index populations, and
\begin{align*}
 D_iz_{j,k} & = D_i (1-2p_j)(1-2q_k), \\
 {\pi_2}(i,j;k,l) & = \begin{cases}
 				p_i(1-p_i)q_k(1-q_k), & i=j, k=l \\
				\frac{1}{2} p_i(1-p_j) q_k(1-q_k) + \frac{1}{2} p_j(1-p_i) q_k(1-q_k), & i\not= j, k=l \\
				\frac{1}{2} p_i(1-p_i) q_k(1-q_l) + \frac{1}{2} p_i(1-p_i) q_l(1-q_k), & i=j, k\not=l \\
				\frac{1}{4} p_i(1-p_j) q_k(1-q_l) + \frac{1}{4} p_i(1-p_j) q_l(1-q_k) & \\
					\hspace{15 pt} + \frac{1}{4} p_j(1-p_i) q_k(1-q_l) + \frac{1}{4} p_j(1-p_i) q_l(1-q_k) , & i\not=j, k\not=l
 			\end{cases}, \\
 H_{i,j} & = \begin{cases} 
 			p_i(1-p_i), & i = j \\ 
			\frac{1}{2}p_i(1-p_j) + \frac{1}{2}p_j(1-p_i),& i \not= j 
		\end{cases}.
\end{align*}

From these, we can compare a large number of two-locus statistics.
In practice, we work with $\sigma_D^2$-like statistics, which are normalized by the $\pi_2$ statistic from one of the populations.


\section{Getting started}

\subsection{Getting and installing \mold}

\mold is packaged and released with \py{moments}, a python program for running analyses based on the allele frequency spectrum.
\py{moments} is available at \url{https://bitbucket.org/simongravel/moments}.
Because \mold continues to be developed and improved, it currently lives on the LD branch of the \py{moments} repository.
For this reason, I recommend cloning the \py{moments} repository and switching to the LD branch.
To do this, in the Terminal navigate to the parent directory where you want to copy moments and run
\py{git clone https://bitbucket.org/simongravel/moments.git}.
To switch to the LD branch, run
\py{git checkout LD}.
And then finally, to install run
\py{sudo python setup.py install}.

\subsubsection{Dependencies}

\py{moments} and \mold depend on a number of Python libraries (We recommend using Python 3).

\begin{enumerate}
\item Absolute dependencies: \py{Python} (version 2.7 or $\geq$ 3.5), \py{numpy} (version $\geq$ 1.2.0), \py{scipy} (version $\geq$ 0.6.0), \py{cython}, \py{pickle}
\item For Plotting, we use \py{matplotlib} (version $\geq$ 0.98.1)
\item For Parsing, we take advantage of \py{hdf5} and \py{scikit-allel} (version $\geq$ 1.2.0) \cite{}
\item For Demography building, we use \py{networkx}
\end{enumerate}

I recommend that you install IPython as well.
The easiest way to obtain all these dependencies is to use a package management system, such as Conda (\url{https://conda.io/en/latest/}) or Enthought (\url{https://www.enthought.com/product/enthought-python-distribution/}).


\subsection{Suggested workflow}

One of Python's strengths is its interactive nature.
When I am first exploring data or writing scripts to build and test models, I often have two windows open: one editing a python script (\py{script.py}) and the other running an IPython session.
That way, I can record my work in the python script and test it as I go.
Using IPython, I can call the magic command \py{\%run script.py}, which applies changes I've made in my python script to the IPython session.
Note that if I've also changed modules that I've loaded, I'll need to reload those as well.
Once I'm happy that I have a usable script, I can call it from the terminal for longer runs of optimization or parsing, using \py{python script.py}.

Note that we will need to import \py{moments.LD} to be able to use it: \py{import moments.LD as mold}.

\section{LDstats objects}

\mold represents two-locus statistics using \py{mold.LDstats} objects, which stores the Hill-Robertson statistics and heterozygosity for one or more populations and for any set of recombination rates.
The simplest way to create an \py{LDstats} object is by defining a set of statistics by hand.
For a single population, the order of the LD statistics is $[\E[D^2], \E[Dz], \E[\pi_2]]$ along with heterozygosity $\E[H]$.
If the statistics are defined using variables \py{D2}, \py{Dz}, \py{pi2}, and \py{H}, we would simply call \py{y = mold.LDstats([[D2, Dz, pi2], [H]], num_pops = 1)}.
For example, if we set \py{D2, Dz, pi2, H = 1e-7, 1e-8, 2e-7, 1e-3} in this example, and then if we print \py{y}, we would see as the output:

\py{Out[1]: LDstats([[1.e-07 1.e-08 2.e-07]], [0.001], num_pops=1, pop_ids=None)}

To see which statistics each value corresponds to, we can call \py{y.names()}, which would output:

\py{Out[2]: (['DD_1_1', 'Dz_1_1_1', 'pi2_1_1_1_1'], ['H_1_1'])}

Typically, we either compute the \py{LDstats} from a demographic model, or we build the object from data.
We'll walk through both in later sections.
First, we'll use build in model functions to explore what information is stored in \py{LDstats} objects, and how to manipulate the objects.

To obtain the expected statistics at equilibrium (steady-state demography), we can call \py{mold.Demographics1D.snm} (snm stands for standard neutral model).
We can specify the per-base population size-scaled mutation rate $\theta = 4N_e\mu$ (default set to $\theta=0.001$) and population size-scaled recombination rates separating loci $\rho=4 N_e r$ (default set to \py{None}).
\mold can handle any number of recombination rates (zero, one, or multiple), and the returned \py{LDstats} object will contain LD statistics for as many recombination rates as you gave it.

To give some examples,\\
\py{In [3]: mold.Demographics1D.snm()}\\
\py{Out[3]: LDstats([], [0.001], num_pops=1, pop_ids=None)}

\py{In [4]: mold.Demographics1D.snm(rho=0)}\\
\py{Out[4]: LDstats([[1.38888889e-07 1.11111111e-07 3.05555556e-07]], [0.001], num_pops=1, pop_ids=None)}

\py{In [5]: mold.Demographics1D.snm(rho=[0,1,2,10], theta=0.01)}\\
\py{Out[5]:}\\
\py{LDstats([[1.38888889e-05 1.11111111e-05 3.05555556e-05]}\\
\py{ [8.59375000e-06 6.25000000e-06 2.81250000e-05]}\\
\py{ [6.25000000e-06 4.16666667e-06 2.70833333e-05]}\\
\py{ [2.01612903e-06 8.06451613e-07 2.54032258e-05]], [0.01], num_pops=1, pop_ids=None)}

In this last example, the four sets of LD statistics correspond to $\rho=0,1,2,$ and $10$, respectively, while expected heterozygosity is only shown a single time ($\E[H]=0.01$).
In each case, \py{num_pops} is automatically set to \py{1}, and because we didn't specify population IDs, \py{pop_ids = None}.
\py{y.LD()} will return just the LD statistics, while \py{y.H()} returns just the heterozygosity statistics.



\section{Parsing and importing data}

\mold can import data and compute LD statistics from either phased or unphased sequencing data.
Typically, data is stored in a VCF formatted file.
We parse the VCF using \py{scikit-allel} to get genotype arrays, which we then iterate over to count two locus haplotype (phased data) or genotype (unphased data) frequencies for pairs of SNPs.
We then use two-locus haplotype or genotype counts to compute statistics in the multi-population Hill-Robertson basis.
If we are binning data based on recombination distance, we will also need a recombination map, and if there are multiple populations, we will need to provide a file that identifies individuals with their population (formats for each are described below).
If we are interested in data from a subset of the full data (say we want to keep only intergenic variants, or want to focus on a particular region), we can provide a bed file that defines the regions or features we should parse.

\subsection{Computing statistics from genotype arrays}
To start simply, we might be interested in computing pairwise LD statistics for a given set of genotypes or between two sets of genotypes.
A genotype array $G$ has size $L\times n$, where $L$ is the number of SNPs and $n$ is the number of sequenced diploid individuals, so that entry $(i,j)$ is the genotype state of individual $j$ at SNP $i$.
Genotype states are either $0$ (homozygous reference), $1$ (heterozygous), or $2$ (homozygous alternate).
\py{mold.Parsing} counts and computes statistics from genotype arrays using the approach described in \citet{Ragsdale2019}.

To get pairwise statistics for each possible pair (all $L(L-1)/2$ of them), use \py{mold.Parsing.compute_pairwise_stats(G)}, where \py{G} is the genotype matrix as described above.
This will output three vectors, each of length $L(L-1)/2$, for $D^2$, $Dz$, and $\pi_2$, in that order.
To convert to a (symmetric) $L\times L$ matrix of pairwise, we can use \py{numpy}'s \py{triu} function.

To get the average Hill-Robertson statistics over all pairwise comparisons for a block of SNPs, use \py{mold.Parsing.compute_average_stats(G)}, which returns the mean values of $D^2$, $Dz$, and $\pi_2$, in that order.

To compute Hill-Robertson statistics between two sets of genotype data, stored in two genotype arrays \py{G1} and \py{G2}, we can similarly call \py{mold.Parsing.compute_pairwise_stats_between(G1, G2)} (which returns vectors of size $L_1L_2$, where $L_i$ is the number of SNPs in genotype array $G_i$) and \py{mold.Parsing.compute_average_stats_between(G1, G2)}.


\subsection{Computing statistics from a VCF file}

In our analyses, we are interested in computing two-locus statistics from genotype data stored in a VCF for pairs of SNPs at varying genetic distances.
To parse a VCF file and output Hill-Robertson statistics, we use \py{mold.Parsing.compute_ld_statistics}.
The only required input for this function is the VCF filename.
Otherwise, there are a number of options and arguments that can be passes to this function.
\begin{itemize}
\item \py{bed_file} : default set to \py{None}. To only parse variants in given regions, we specify those regions in a bed file.
\item \py{rec_map_file} : default set to \py{None}. If we pass a recombination map filename, we can specify the formatting of the recombination map file (see subsection below).
\item \py{pop_file} : default set to \py{None}. If \py{None}, it uses data from all individuals as a single population. If we pass a population file, we can specify which populations to parse using \py{pops=[pop1, pop2, ...]}.
\item \py{bp_bins} : default set to \py{None}. If we do not pass a recombination map, we will parse the data based on base pair distance between SNPs. Here, we pass a list of bin edges. For example, to parse data into bins of $(0,10\text{ kb}]$, $(10\text{ kb},20\text{ kb}]$, and $(20\text{ kb},30\text{ kb}]$, we would set \py{bp_bins = [0, 10000, 20000, 30000]}.
\item \py{use_genotypes} : default set to \py{True}, which we used with unphased data. Set to \py{False} for phased data.
\item \py{stats_to_compute} : default set to \py{None}. If \py{None}, computes all statistics in the multi-population H-R basis. We can also specify just a subset of these statistics, if desired.
\item \py{report} : default set to \py{True}. If \py{True}, outputs progress report as it parses the VCF file.
\end{itemize}

In the simplest case without a recombination map, we can compute statistics from a VCF file based on physical (bp) distance.
To do this, for a single population, we only need to specify the VCF filename and bin edges in base pairs:\\
\py{ld_stats = mold.Parsing.compute_ld_statistics('path/to/vcf/file.vcf.gz', bp_bins=[0, 10000, 20000, 30000])}.

\subsection{Multiple populations}

To parse data for multiple populations, we need to also include a file that tells us which population each individual belongs to.
In the population file, each row corresponds to an individual in the VCF file (individual names must match to those labeled in the VCF header).
In each row, the first column is the individual ID, and the second column is the population name.
Additional columns are ignored.
We could then call \py{ld_stats = mold.Parsing.compute_ld_statistics('path/to/vcf/file.vcf.gz', pop_file='path/to/pop/file.txt', pops=[pop1, pop2, pop3], bp_bins=[0, 10000, 20000, 30000])}.

\subsection{Using a recombination map}

Because recombination rates can vary along the genome, we often want to parse two-locus data by the genetic instead of physical distance separating SNPs.
To use a recombination map to parse data, we specify the file path and name using \py{rec_map_file}.
In the recombination map file, the first column corresponds to physical positions, and other columns correspond to the genetic position in either Morgans or cM.
If there are multiple maps in the file, we can specify the map we want to use by the map name in the header.

For example, the first and last few lines of a set of recombination maps of chromosome 22 for build 37 (available here: \url{https://www.well.ox.ac.uk/~anjali/AAmap/}) are\\\small
\py{
"Physical_Pos" "deCODE" "COMBINED_LD" "YRI_LD" "CEU_LD" "AA_Map" "African_Enriched" "Shared_Map"}\\\py{
16051347 0 0 0 0 0 0 0}\\\py{
16052618 0 0.0099 0.0083 0.0163 0 0 0}\\\py{
16053624 0 0.0177 0.0148 0.0293 0 0 0}\\\py{
16053659 0 0.0179 0.0151 0.0297 0 0 0}\\\py{
16053758 0 0.0187 0.0157 0.031 0 0 0}\\\py{
...}\\\py{
51217134 55.5922 73.6005 75.6968 72.5384 68.9516 67.8206 54.8248}\\\py{
51219006 55.5922 73.6017 75.6982 72.5398 68.9516 67.8206 54.8248}\\\py{
51222100 55.5922 73.6037 75.7006 72.5421 68.9516 67.8206 54.8248}\\\py{
51223637 55.5922 73.6047 75.7018 72.5433 68.9516 67.8206 54.8248}\\\py{
51229805 55.5922 73.6088 75.7068 72.5479 68.9516 67.8206 54.8248}\\
\normalsize
Cumulative distances are given in cM.
If we want to use the African American admixutre map (``AA\_Map'', \cite{Hinch2011}) from this file, 
setting recombination bins using \py{r_bins} as, for example,\\
\py{r_bins = [0, 1e-5, 2e-5, 5e-5, 1e-4, 2e-4, 1e-3]}\\
we call:\\
\py{ld_stats = mold.Parsing.compute_ld_statistics('path/to/vcf/file.vcf.gz', rec_map_file='path/to/rec/map/file', map_name='AA_Map', map_sep=' ', cM=True, r_bins=r_bins)}.\\
\py{map_sep} by default is set to tab separation, so we'll need tell the parsing function that this map is separated by spaces.

\subsection{Parsing example}

\comment{Explain \py{msprime_two_pop_parsing.py}}

\subsection{Creating bootstrap datasets}
\comment{to do}

\section{Specifying a model}

In \mold, we want to build and test demographic models by computing LD statistics for a specified model.
\mold allows two ways to specify demographic models, either through direct manipulation of \py{LDstats} objects, or by defining a demography through a graphical structure.
In this section, we'll describe and give examples for each.
We'll start with the direct manipulation of \py{LDstats} objects to give intuition about how \mold computes LD statistics under different demographic scenarios.
For more than just a couple populations, it is far easier to implement models using the Demography module, which builds the demography from a user-defined directed graph.

\subsection{Implementation}

Implementation should be familiar to \dadi and \py{moments} users.
Once we have defined a \py{LDstats} object, we can perform demographic functions and manipulations on it and integrate it forward in time.
We can start by specifying the initial distribution, before applying demographic events:\\
\py{y = mold.Demographics1D.snm(rho = [0,1,10], theta=0.001)}.\\
\py{y} is now the single population steady-state set of LD statistics for the specified $\rho$ and $\theta$ parameters.

From here, we can integrate forward in time with a specified relative population size, or we can split the population into two daugher populations.
To integrate forward in time, with a single population, we call:\\
\py{y.integrate([nu], T, rho=[0,1,10], theta=0.001)},\\
where \py{nu} is a relative population size and \py{T} is the integration time (in genetic units).
To split into two, we call:\\
\py{y = y.split(1)}.\\
In the \py{split} function, we specify the population number we want to split, and a new population is appended to the end of the population list.
For example, if \py{y} currently has two populations, and we want to split population 1 into 1A and 1B, we call \py{y.split(1)}, which returns a new \py{LDstats} object with populations ordered as (1A, 2, 1B).

Below, I write out some examples for defining demographic models (a bottleneck model with recent growth, an isolation with migration model, and the Gutenkunst out-of-Africa model).

\subsection{The \texttt{Demography} builder}

Manual specification of demographic functions gets difficult with multiple populations, particularly for more than two or three populations where events in different branches of the demography could switch their order of occurrences.

\subsection{Units}
\begin{enumerate}
\item mutation rate
\item recombination rate
\item unit of time
\item $N_e$ and $N_{ref}$
\end{enumerate}

\clearpage

\begin{lstlisting}[caption={\textbf{Bottleneck model:} At time \py{T} in the past, an equilibrium population goes through a bottleneck of depth \py{nuB}, recovering to relative size \py{nuF} through exponential growth. In all examples listed here, we need to \py{import numpy as np} and \py{import moments.LD as mold}.}, float, label={lst:bottleneck}]
def bottleneck_growth(params, rho=None, theta=0.001):
    """
    Instantaneous bottleneck to size nuB
    followed by exponential growth to size nuF over time T
    """
    nuB, nuF, T = params
    nu_func = lambda t: [nuB * np.exp(np.log(nuF/nuB) 
    			   * t / T)]

    y = mold.Demographics1D.snm(rho=rho, theta=theta)
    y.integrate(nu_func, T, rho=rho, theta=theta)

    return y
\end{lstlisting}

\begin{lstlisting}[caption={\textbf{IM model:}}, float, label={lst:IM}]
def IM():
    return y
\end{lstlisting}

\begin{lstlisting}[caption={\textbf{Out of Africa model:}}, float, label={lst:IM}]
def ooa():
    return y
\end{lstlisting}


\begin{lstlisting}[caption={\textbf{Out of Africa Demography:}}, float, label={lst:IM}]
def ooa_demo():
    return y
\end{lstlisting}

\begin{lstlisting}[caption={\textbf{Archaic Hominin Demography:}}, float, label={lst:IM}]
def archaics_demo():
    return y
\end{lstlisting}


\clearpage

\section{Simulation and fitting the model}

\subsection{Running the model}

\subsection{Comparing model to data}

\subsubsection{Likelihoods}


\subsubsection{Fitting}

\begin{enumerate}
\item Parameter bounds
\item Fixed parameters
\item Other options (there are a lot more now)
\item optimizer choice
\end{enumerate}

\subsection{Uncertainty analysis}


\section{Plotting}

\subsection{Visualizing LD curves}


\subsection{Residuals}
Based on expectations, observations, and covariances


\section{The full two-locus frequency spectrum}

\subsection{\texttt{moments.TwoLocus}}

\subsubsection{Specifying models}

\subsubsection{Parameters}

\subsubsection{Selection}


\section{Frequently asked questions}

\begin{enumerate}

\item What if I'm having issues running or installing this program?

Bug: issues

Bigger issues or difficulties: email

\item How do I cite \mold?

\end{enumerate}



\section{Acknowledgements}
\py{moments}, \mold, and their manuals (and indeed much of my scientific course and accomplishments) are greatly indebted to Ryan Gutenkunst.
Not only are these programs modeled off of \dadi's interface and functionality, but also my general approach to writing, testing, and using scientific software has largely been guided and influenced by Ryan.
I cannot overstate my gratitude to Ryan for making \dadi open source and accessible, and more personally for his mentorship over the years (if you're reading this Ryan, a gigantic ``Thank you!'').



\bibliography{manualLD}
\bibliographystyle{apalike}

\end{document}
