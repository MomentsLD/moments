\documentclass[11pt]{article}

\usepackage[margin=2.5cm]{geometry}

\usepackage{amsmath,amsfonts,amssymb}
\usepackage{url}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{hyperref}

\usepackage[round]{natbib}
\renewcommand{\cite}{\citep}
\setlength{\bibhang}{0pt}

\setlength{\parindent}{0pt}
\setlength{\parskip}{10pt}

%\bibliographystyle{plos2009}

\usepackage{xspace}
\newcommand{\dadi}{$\partial$a$\partial$i\xspace}
\newcommand{\bolddadi}{$\boldsymbol{\partial}$a$\boldsymbol{\partial}$i\xspace}
\newcommand{\Nref}{\ensuremath{N_\text{ref}}\xspace}
\newcommand{\ms}{\emph{ms}\xspace}
\usepackage{color}
\newcommand{\comment}[1]{{\color{blue}APR: #1}}

\newcommand{\mold}{\texttt{mold}\xspace}

\usepackage{listings}
\lstset{
basicstyle=\ttfamily,
language=Python,
showstringspaces=False,
aboveskip=0pt,
captionpos=b,
belowskip=0pt
}
\newcommand{\py}[1]{\lstinline[breaklines=true,language=Python, showstringspaces=False]@#1@}
\newcommand{\ccode}[1]{\lstinline[breaklines=true,language=C, showstringspaces=False]@#1@}
\newcommand{\shell}[1]{\lstinline[breaklines=true, language=csh, showstringspaces=False]@#1@}

\newcommand{\E}{\mathbb{E}}

% For calibration, lines can be 60 characters long in
% lstlistings.
%\begin{lstlisting}
%*******************************************************
%\end{lstlisting}


\begin{document}
\title{\texttt{Moments LD} user manual\\
\normalsize  Corresponding to version 0.0.1}
\author{Aaron Ragsdale \\
Contact: aaron.ragsdale@mail.mcgill.ca}
\date{\today}
\maketitle

\tableofcontents

\clearpage

\renewcommand*{\lstlistlistingname}{Example code}
\lstlistoflistings

\clearpage

\section{Introduction to \mold}

Welcome to \texttt{moments.LD}, a program for simulating linkage disequilibrium statistics.
\texttt{moments.LD}, or \mold, can compute a large set of informative LD statistics for many populations, and performs likelihood-based demographic inference using those statistics.

There are three primary features of \mold to enable LD-based demographic inference: reading and parsing data, building demographic models, and inferring the parameters of those models by comparing model predictions to data.
Typically, we use biallelic SNP data, along with a recombination map, to compute two-locus statistics over a range of genetic distances.
We then use \mold to compute expectations for those statistics under the demographic models we want to test, which can include multiple populations with variable migration, splits and mergers, and population size changes.
Using a likelihood-based inference approach, we optimize those models to find the set of parameters that best fit the data.

I've tried to make parsing data and defining demographic models as painless as possible, though the complexity of the program does require some amount of script-writing and interaction.
Luckily, \mold is written in Python, a friendly and powerful programming language.
If you are already familiar with \dadi or \emph{moments}, or Python in general, you are in a good position to dive right in to \mold.
If you have limited Python experience, this manual should provide the background and examples to get you up to speed and productive with \mold.

%Finally, \mold is a living, breathing, evolving \comment{thing}

\subsection{Getting help and helping us}

Undoubtedly, there will be bugs.
If you find a bug in \mold, or more generally if you find certain aspects of the program to be unintuitive or difficult to use, I would appreciate the feedback.
Please submit a bug report at \url{https://bitbucket.org/simongravel/moments/issues}, and I will try to address the issue in a timely manner.
Similarly, if you have suggestions for improved functionality or feature requests, those can be submitted in the issues as well or you can contact me directly.

As we do our own research, \textit{moments} and \mold are constantly improving.
Our philosophy is to include any code we develop for our own projects that may useful to others.
If you develop \textit{Moments}-related code that you think might be useful to others, please let me know so I can include it with the main distribution.

\section{LD statistics}

Patterns of linkage disequilibrium (LD) are informative about evolutionary history, for example for inferring recent admixture events and population size changes or localizing regions of the genome that have experienced recent selective events.
LD is commonly measured as the covariance (or correlation) of alleles co-occurring on a haplotype.
The covariance ($D$) is
\begin{align*}
D = \text{Cov}(A,B) & = f_{AB} - pq \\ & = f_{AB}f_{ab} - f_{Ab}f_{aB},
\end{align*}
and the correlation ($r$) is
\begin{align*}
r = \frac{D}{\sqrt{p(1-p)q(1-q)}} .
\end{align*}
We think of expectations of these quantities as though we average over many realizations of the same evolutionary process, but in reality we have only a single observation for any given pair of SNPs.
Therefore in practice we take the averages of LD statistics over many independent pairs of SNPs.

$\E[D]$ is zero genome wide, so LD is often measured by the variance of $D$ ($\E[D^2$) or the square correlation ($r^2$), where
\begin{align*}
r^2 = \frac{D^2}{p(1-p)q(1-q)}.
\end{align*}
Because it is difficult to compute expectations for $\E[r^2]$ under even simple evolutionary scenarios, and because it is difficult to accurately estimate $\widehat{r^2}$ from data, we use $\E[D^2$ and related statistics to compare model predictions for LD to data.

\subsection{Hill-Robertson statistics}

\citet{Hill1968} introduced a recursion for $\E[D^2]$ that allows for variable recombination rate between loci and population size changes over time.
To solve for $\E[D^2]$, this system requires additional LD statistics, which we call $Dz = D(1-2p)(1-2q)$ and $\pi_2 = p(1-p)q(1-q)$, where $p$ and $q$ are the allele frequencies at the left and right loci, respectively.
This system also relies on heterozygosity ($H$), so from this system we can compute the vector of statistics
$$y=\begin{pmatrix} \E[D^2] \\ \E[Dz] \\ \E[\pi_2] \\ \E[H] \end{pmatrix}.$$

Instead of computing $\E[r^2]$, which is an expectation of ratios, \citet{Hill1968} and \citet{Ohta1971} studied the related statistic $\sigma_D^2 = \frac{\E[D^2]}{\E[\pi_2]}$.
This statistic has the advantage that its expectation can be computed from the Hill-Robertson recursion, and we can accurately compute it from either phased or unphased data.

\subsection{Multi-population LD statistics}

In \citet{Ragsdale2018}, we extended the Hill-Robertson system to consider LD statistics for multiple populations.
Here, we can model population size changes, splits, mergers, and migration events (pulse or continuous).
The statistics we consider take the form
$$
\mathbf{z} = \begin{pmatrix} 
\E[D_i D_j] \\
\E[D_i z_{j,k}] \\
\E[{\pi_2}(i,j;k,l)] \\
\E[H_{i,j}]
\end{pmatrix},
$$

where $i,j,k,l$ index populations, and
\begin{align*}
 D_iz_{j,k} & = D_i (1-2p_j)(1-2q_k), \\
 {\pi_2}(i,j;k,l) & = \begin{cases}
 				p_i(1-p_i)q_k(1-q_k), & i=j, k=l \\
				\frac{1}{2} p_i(1-p_j) q_k(1-q_k) + \frac{1}{2} p_j(1-p_i) q_k(1-q_k), & i\not= j, k=l \\
				\frac{1}{2} p_i(1-p_i) q_k(1-q_l) + \frac{1}{2} p_i(1-p_i) q_l(1-q_k), & i=j, k\not=l \\
				\frac{1}{4} p_i(1-p_j) q_k(1-q_l) + \frac{1}{4} p_i(1-p_j) q_l(1-q_k) & \\
					\hspace{15 pt} + \frac{1}{4} p_j(1-p_i) q_k(1-q_l) + \frac{1}{4} p_j(1-p_i) q_l(1-q_k) , & i\not=j, k\not=l
 			\end{cases}, \\
 H_{i,j} & = \begin{cases} 
 			p_i(1-p_i), & i = j \\ 
			\frac{1}{2}p_i(1-p_j) + \frac{1}{2}p_j(1-p_i),& i \not= j 
		\end{cases}.
\end{align*}

From these, we can compare a large number of two-locus statistics.
In practice, we work with $\sigma_D^2$-like statistics, which are normalized by the $\pi_2$ statistic from one of the populations.


\section{Getting started}

\subsection{Getting and installing \mold}

\mold is packaged and released with \py{moments}, a python program for running analyses based on the allele frequency spectrum.
\py{moments} is available at \url{https://bitbucket.org/simongravel/moments}.
Because \mold continues to be developed and improved, it currently lives on the LD branch of the \py{moments} repository.
For this reason, I recommend cloning the \py{moments} repository and switching to the LD branch.
To do this, in the Terminal navigate to the parent directory where you want to copy moments and run
\py{git clone https://bitbucket.org/simongravel/moments.git}.
To switch to the LD branch, run
\py{git checkout LD}.
And then finally, to install run
\py{sudo python setup.py install}.

\subsubsection{Dependencies}

\py{moments} and \mold depend on a number of Python libraries (We recommend using Python 3).

\begin{enumerate}
\item Absolute dependencies: \py{Python} (version 2.7 or $\geq$ 3.5), \py{numpy} (version $\geq$ 1.2.0), \py{scipy} (version $\geq$ 0.6.0), \py{cython}, \py{pickle}
\item For Plotting, we use \py{matplotlib} (version $\geq$ 0.98.1)
\item For Parsing, we take advantage of \py{hdf5} and \py{scikit-allel} (version $\geq$ 1.2.0) \cite{}
\item For Demography building, we use \py{networkx}
\end{enumerate}

I recommend that you install IPython as well.
The easiest way to obtain all these dependencies is to use a package management system, such as Conda (\url{https://conda.io/en/latest/}) or Enthought (\url{https://www.enthought.com/product/enthought-python-distribution/}).


\subsection{Suggested workflow}

One of Python's strengths is its interactive nature.
When I am first exploring data or writing scripts to build and test models, I often have two windows open: one editing a python script (\py{script.py}) and the other running an IPython session.
That way, I can record my work in the python script and test it as I go.
Using IPython, I can call the magic command \py{\%run script.py}, which applies changes I've made in my python script to the IPython session.
Note that if I've also changed modules that I've loaded, I'll need to reload those as well.
Once I'm happy that I have a usable script, I can call it from the terminal for longer runs of optimization or parsing, using \py{python script.py}.

Note that we will need to import \py{moments.LD} to be able to use it: \py{import moments.LD as mold}.

\section{LDstats objects}

\mold represents two-locus statistics using \py{mold.LDstats} objects, which stores the Hill-Robertson statistics and heterozygosity for one or more populations and for any set of recombination rates.
The simplest way to create an \py{LDstats} object is by defining a set of statistics by hand.
For a single population, the order of the LD statistics is $[\E[D^2], \E[Dz], \E[\pi_2]]$ along with heterozygosity $\E[H]$.
If the statistics are defined using variables \py{D2}, \py{Dz}, \py{pi2}, and \py{H}, we would simply call \py{y = mold.LDstats([[D2, Dz, pi2], [H]], num_pops = 1)}.
For example, if we set \py{D2, Dz, pi2, H = 1e-7, 1e-8, 2e-7, 1e-3} in this example, and then if we print \py{y}, we would see as the output:

\py{Out[1]: LDstats([[1.e-07 1.e-08 2.e-07]], [0.001], num_pops=1, pop_ids=None)}

To see which statistics each value corresponds to, we can call \py{y.names()}, which would output:

\py{Out[2]: (['DD_1_1', 'Dz_1_1_1', 'pi2_1_1_1_1'], ['H_1_1'])}

Typically, we either compute the \py{LDstats} from a demographic model, or we build the object from data.
We'll walk through both in later sections.
First, we'll use build in model functions to explore what information is stored in \py{LDstats} objects, and how to manipulate the objects.

To obtain the expected statistics at equilibrium (steady-state demography), we can call \py{mold.Demographics1D.snm} (snm stands for standard neutral model).
We can specify the per-base population size-scaled mutation rate $\theta = 4N_e\mu$ (default set to $\theta=0.001$) and population size-scaled recombination rates separating loci $\rho=4 N_e r$ (default set to \py{None}).
\mold can handle any number of recombination rates (zero, one, or multiple), and the returned \py{LDstats} object will contain LD statistics for as many recombination rates as you gave it.

To give some examples,\\
\py{In [3]: mold.Demographics1D.snm()}\\
\py{Out[3]: LDstats([], [0.001], num_pops=1, pop_ids=None)}

\py{In [4]: mold.Demographics1D.snm(rho=0)}\\
\py{Out[4]: LDstats([[1.38888889e-07 1.11111111e-07 3.05555556e-07]], [0.001], num_pops=1, pop_ids=None)}

\py{In [5]: mold.Demographics1D.snm(rho=[0,1,2,10], theta=0.01)}\\
\py{Out[5]:}\\
\py{LDstats([[1.38888889e-05 1.11111111e-05 3.05555556e-05]}\\
\py{ [8.59375000e-06 6.25000000e-06 2.81250000e-05]}\\
\py{ [6.25000000e-06 4.16666667e-06 2.70833333e-05]}\\
\py{ [2.01612903e-06 8.06451613e-07 2.54032258e-05]], [0.01], num_pops=1, pop_ids=None)}

In this last example, the four sets of LD statistics correspond to $\rho=0,1,2,$ and $10$, respectively, while expected heterozygosity is only shown a single time ($\E[H]=0.01$).
In each case, \py{num_pops} is automatically set to \py{1}, and because we didn't specify population IDs, \py{pop_ids = None}.
\py{y.LD()} will return just the LD statistics, while \py{y.H()} returns just the heterozygosity statistics.



\section{Parsing and importing data}

\begin{enumerate}
\item \mold can create an LDstats object given a vcf file (and optionally a recombination map, mask files, and population files), for either phased or unphased data
\item Or given a genotype array
\end{enumerate}




\subsection{Estimating two-locus statistics from data}
\cite{Ragsdale2019}


\subsection{Parsing data from a genotype matrix}

\begin{enumerate}
\item All loci, not caring about recombination distance (say, all unlinked loci)
\item with the recombination distance between snps known
\end{enumerate}

\subsection{Parsing a VCF file}


\subsubsection{Using a recombination map}


\subsubsection{Restricting based on features}


\subsection{Creating bootstrap datasets}

\clearpage
\begin{lstlisting}[caption={\textbf{Parsing:} Example of parsing data generated by msprime}, float, label={lst:bottleneck}]
this is a script dot py
\end{lstlisting}

\clearpage

\section{Specifying a model}

General overview of demographic models

\begin{enumerate}
\item Attributes of LDstats objects used for building models
\item Splitting, marginalization, swapping
\item Admixture, merging
\end{enumerate}

\subsection{Implementation}

Manual input of demographic model (gets difficult with more than two populations)

\subsection{The \texttt{Demography} builder}

\subsection{Units}
\begin{enumerate}
\item mutation rate
\item recombination rate
\item unit of time
\item $N_e$ and $N_{ref}$
\end{enumerate}

\clearpage
\begin{enumerate}
\item With old approach: bottleneck with growth, IM model
\item With demography: Gutenkunst model, Archaic model (w/ denisovans, neand, papuan, EA, EU, Afr)
\end{enumerate}

\begin{lstlisting}[caption={\textbf{Bottleneck:} At time \py{TF} + \py{TB} in the past, an equilibrium population goes through a bottleneck of depth \py{nuB}, recovering to relative size \py{nuF}.}, float, label={lst:bottleneck}]
def bottleneck(params, ns):
    nuB, nuF, T = params
    nu_func = lambda t: [nuB * numpy.exp(numpy.log(nuF/nuB) 
    			   * t / T)]

    sts = moments.LinearSystem_1D.steady_state_1D(ns[0])
    fs = moments.Spectrum(sts)
    fs.integrate(nu_func, T)

    return fs
\end{lstlisting}

\clearpage

\section{Simulation and fitting the model}

\subsection{Running the model}

\subsection{Comparing model to data}

\subsubsection{Likelihoods}


\subsubsection{Fitting}

\begin{enumerate}
\item Parameter bounds
\item Fixed parameters
\item Other options (there are a lot more now)
\item optimizer choice
\end{enumerate}

\subsection{Uncertainty analysis}


\section{Plotting}

\subsection{Visualizing LD curves}


\subsection{Residuals}
Based on expectations, observations, and covariances


\section{The full two-locus frequency spectrum}

\subsection{\texttt{moments.TwoLocus}}

\subsubsection{Specifying models}

\subsubsection{Parameters}

\subsubsection{Selection}


\section{Frequently asked questions}

\comment{No one has really asked me questions yet, but here are my own quandaries and answers:}

\begin{enumerate}

\item 

\item How do I cite \mold?

\item What if I'm having issues running this program?

Bug: issues

Bigger issues or difficulties: email

\end{enumerate}



\section{Acknowledgements}
\py{moments}, \mold, and their manuals (and indeed much of my scientific course and accomplishments) are greatly indebted to Ryan Gutenkunst.
Not only are these programs modeled off of \dadi's interface and functionality, but also my general approach to writing, testing, and using scientific software has largely been guided and influenced by Ryan.
I cannot overstate my gratitude to Ryan for making \dadi open source and accessible, and more personally for his mentorship over the years (if you're reading this Ryan, a gigantic ``Thank you!'').



\bibliography{manualLD}
\bibliographystyle{apalike}

\end{document}
